{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18640/1330914320.py:12: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n",
    "import findspark\n",
    "import zipfile\n",
    "import os\n",
    "from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/spark\n",
      "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/11/09 14:51:22 WARN Utils: Your hostname, r resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/09 14:51:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 14:51:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import findspark\n",
    "# from pyspark.sql import SparkSession\n",
    "print(\"SPARK_HOME:\", os.environ.get('SPARK_HOME'))\n",
    "print(\"JAVA_HOME:\", os.environ.get('JAVA_HOME'))\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"sparksql\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext sparksql_magic\n",
    "# %config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3493: UserWarning: Config option `max_num_rows` not recognized by `SparkSql`.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparksql_magic\n",
    "%config SparkSql.max_num_rows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general paths\n",
    "DATA = '/home/r/git_projekty/sparkTraining/data'\n",
    "PACKAGED_DATA = '/home/r/git_projekty/sparkTraining/data/packaged'\n",
    "UNPACKAGED_DATA = '/home/r/git_projekty/sparkTraining/data/unpackaged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = '/home/r/pyspark/sparkTraining/New_Query_2025-08-01_12_01pm_2025_08_08.csv'\n",
    "foler_zip = '/home/r/pgetl/sparkTraining/data.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path: str, extract_to: str):\n",
    "    try:\n",
    "        if not os.path.exists(extract_to):\n",
    "            os.makedirs(extract_to)\n",
    "            print(f\"Created directory: {extract_to}\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"Extracted all files to: {extract_to}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: The file {zip_path} was not found. {e}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error: The file {zip_path} is not a zip file or it is corrupted. {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r/git_projekty/sparkTraining/data/packaged\n",
      "Created directory: /home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets\n",
      "Extracted all files to: /home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets\n",
      "Created directory: /home/r/git_projekty/sparkTraining/data/unpackaged/csecicids2018\n",
      "Extracted all files to: /home/r/git_projekty/sparkTraining/data/unpackaged/csecicids2018\n",
      "Created directory: /home/r/git_projekty/sparkTraining/data/unpackaged/game-recommendations-on-steam\n",
      "Extracted all files to: /home/r/git_projekty/sparkTraining/data/unpackaged/game-recommendations-on-steam\n",
      "Created directory: /home/r/git_projekty/sparkTraining/data/unpackaged/us-wildfire-records-6th-edition\n",
      "Extracted all files to: /home/r/git_projekty/sparkTraining/data/unpackaged/us-wildfire-records-6th-edition\n"
     ]
    }
   ],
   "source": [
    "packaged_data_path = PACKAGED_DATA\n",
    "print( packaged_data_path)\n",
    "files_to_unpack = os.listdir(packaged_data_path) #'csecicids2018.zip'\n",
    "data = '/home/r/git_projekty/sparkTraining/data'\n",
    "for file_to_unpack in files_to_unpack:\n",
    "    # print(file_to_unpack,'++')\n",
    "    if file_to_unpack.endswith('.zip'):\n",
    "            extract_to = file_to_unpack.replace('.zip', '')\n",
    "            # print( extract_to)\n",
    "    if extract_to.startswith('.'):\n",
    "        # print( extract_to, '========')\n",
    "        extract_to = extract_to.replace('.', '')\n",
    "    #     print( extract_to, '--')\n",
    "    # print( extract_to, '<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "    extract_zip_file(os.path.join(packaged_data_path, file_to_unpack), os.path.join(UNPACKAGED_DATA, extract_to))\n",
    "# extract_to = file_to_unpack.replace('.zip', '')\n",
    "# extract_zip_file(os.path.join(packaged_data_path, file_to_unpack), os.path.join(packaged_data_path, extract_to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r/git_projekty/sparkTraining/data/packaged/csesisids2018.zip\n",
      "False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unziptar(folder_zip, output_folder):\n",
    "    with tarfile.open(folder_zip, 'r:gz') as tar:\n",
    "        tar.extractall(path=output_folder)\n",
    "\n",
    "    print(f\"Plik {folder_zip} został rozpakowany do katalogu {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_folder = '/home/r/pgetl/sparkTraining/extracted_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unziptar(foler_zip, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pierwsze 5 wierszy ---\n",
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "|     id|               date|client_id|card_id| amount|         use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|\n",
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "|7475327|2010-01-01 00:01:00|     1556|   2972|$-77.00|Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|\n",
      "|7475328|2010-01-01 00:02:00|      561|   4575| $14.57|Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL|\n",
      "|7475329|2010-01-01 00:02:00|     1129|    102| $80.00|Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|\n",
      "|7475331|2010-01-01 00:05:00|      430|   2860|$200.00|Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|\n",
      "|7475332|2010-01-01 00:06:00|      848|   3915| $46.41|Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|\n",
      "+-------+-------------------+---------+-------+-------+-----------------+-----------+-------------+--------------+-------+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- Schemat (struktura danych) ---\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- card_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- mcc: string (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Definicja ścieżki do pliku\n",
    "csv_file_path = \"/home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets/transactions_data.csv\"\n",
    "\n",
    "def read_csv_with_spark(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Wczytuje plik CSV do DataFrame Spark z określonymi opcjami.\n",
    "\n",
    "    :param spark: SparkSession\n",
    "    :param file_path: Ścieżka do pliku CSV\n",
    "    :return: DataFrame Spark\n",
    "    \"\"\"\n",
    "    df = spark.read.csv(\n",
    "        file_path,\n",
    "        header=True,       # Użyj pierwszej linii jako nagłówków kolumn\n",
    "        inferSchema=False,  # Spróbuj automatycznie wykryć typy danych (np. Int, String, Date)\n",
    "        sep=\",\",           # Określ separator (domyślnie przecinek, ale można zmienić na ';', '\\t' itd.)\n",
    "        encoding=\"UTF-8\"   # Określ kodowanie pliku\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_csv = read_csv_with_spark(spark, csv_file_path)\n",
    "\n",
    "# 3. Wyświetlenie pierwszych wierszy i schematu\n",
    "print(\"--- Pierwsze 5 wierszy ---\")\n",
    "df_csv.show(5)\n",
    "\n",
    "print(\"--- Schemat (struktura danych) ---\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.createOrReplaceTempView(\"transactionsdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">id</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">client_id</td><td style=\"font-weight: bold\">card_id</td><td style=\"font-weight: bold\">amount</td><td style=\"font-weight: bold\">use_chip</td><td style=\"font-weight: bold\">merchant_id</td><td style=\"font-weight: bold\">merchant_city</td><td style=\"font-weight: bold\">merchant_state</td><td style=\"font-weight: bold\">zip</td><td style=\"font-weight: bold\">mcc</td><td style=\"font-weight: bold\">errors</td></tr><tr><td>7475327</td><td>2010-01-01 00:01:00</td><td>1556</td><td>2972</td><td>$-77.00</td><td>Swipe Transaction</td><td>59935</td><td>Beulah</td><td>ND</td><td>58523.0</td><td>5499</td><td>null</td></tr><tr><td>7475328</td><td>2010-01-01 00:02:00</td><td>561</td><td>4575</td><td>$14.57</td><td>Swipe Transaction</td><td>67570</td><td>Bettendorf</td><td>IA</td><td>52722.0</td><td>5311</td><td>null</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from transactionsdata limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pierwsze 5 wierszy ---\n",
      "+------+--------------------+--------------------+\n",
      "|app_id|         description|                tags|\n",
      "+------+--------------------+--------------------+\n",
      "| 13500|Enter the dark un...|[Action, Adventur...|\n",
      "| 22364|                    |            [Action]|\n",
      "|113020|Monaco: What's Yo...|[Co-op, Stealth, ...|\n",
      "|226560|Escape Dead Islan...|[Zombies, Adventu...|\n",
      "|249050|Dungeon of the En...|[Roguelike, Strat...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- Schemat (struktura danych) ---\n",
      "root\n",
      " |-- app_id: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Definicja ścieżki do pliku\n",
    "json_file_path = \"/home/r/git_projekty/sparkTraining/data/unpackaged/game-recommendations-on-steam/games_metadata.json\"\n",
    "\n",
    "def read_json_with_spark(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Wczytuje plik CSV do DataFrame Spark z określonymi opcjami.\n",
    "\n",
    "    :param spark: SparkSession\n",
    "    :param file_path: Ścieżka do pliku CSV\n",
    "    :return: DataFrame Spark\n",
    "    \"\"\"\n",
    "    df = spark.read.json(\n",
    "        file_path,\n",
    "        multiLine=False,    # Wczytaj JSON wieloliniowy, jeśli plik jest w takim formacie\n",
    "        encoding=\"UTF-8\"   # Określ kodowanie pliku\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_json = read_json_with_spark(spark, json_file_path)\n",
    "\n",
    "# 3. Wyświetlenie pierwszych wierszy i schematu\n",
    "print(\"--- Pierwsze 5 wierszy ---\")\n",
    "df_json.show(5)\n",
    "\n",
    "print(\"--- Schemat (struktura danych) ---\")\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.createOrReplaceTempView(\"trainfraudlabels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">app_id</td><td style=\"font-weight: bold\">description</td><td style=\"font-weight: bold\">tags</td></tr><tr><td>13500</td><td>Enter the dark underworld of Prince of Persia Warrior Within, the sword-slashing sequel to the critically acclaimed Prince of Persia: The Sands of Time™. Hunted by Dahaka, an immortal incarnation of Fate seeking divine retribution, the Prince embarks upon a path of both carnage and mystery to defy his preordained death.</td><td>[&#x27;Action&#x27;, &#x27;Adventure&#x27;, &#x27;Parkour&#x27;, &#x27;Third Person&#x27;, &#x27;Great Soundtrack&#x27;, &#x27;Singleplayer&#x27;, &#x27;Platformer&#x27;, &#x27;Time Travel&#x27;, &#x27;Atmospheric&#x27;, &#x27;Classic&#x27;, &#x27;Hack and Slash&#x27;, &#x27;Time Manipulation&#x27;, &#x27;Gore&#x27;, &#x27;Fantasy&#x27;, &#x27;Story Rich&#x27;, &#x27;Dark&#x27;, &#x27;Open World&#x27;, &#x27;Controller&#x27;, &#x27;Dark Fantasy&#x27;, &#x27;Puzzle&#x27;]</td></tr><tr><td>22364</td><td></td><td>[&#x27;Action&#x27;]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from trainfraudlabels limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files_to_df(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ładuje pliki JSON z podanego folderu do DataFrame, używając istniejącej sesji Spark.\n",
    "    Args:\n",
    "        spark: Aktywna sesja Spark.\n",
    "        folder_path: Ścieżka do folderu zawierającego pliki JSON.\n",
    "    Returns:\n",
    "        DataFrame PySpark z załadowanymi danymi lub pusty DataFrame, jeśli nie znaleziono pasujących plików.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Wczytanie wszystkich plików JSON z folderu\n",
    "        # df = spark.read.json(f\"{folder_path}/*.json\")\n",
    "        df = spark.read.json(f\"{file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił błąd podczas ładowania plików JSON: {e}\")\n",
    "        # Zwrócenie pustego DataFrame w przypadku błędu\n",
    "        return spark.createDataFrame([], StructType([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets/train_fraud_labels.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 14:18:57 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 8]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.$anonfun$apply$1(FileFormat.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1$$Lambda$3019/0x000070c6ece7f588.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:197)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$2708/0x000070c6ecd8f930.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2709/0x000070c6ecd8fd10.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "25/11/09 14:18:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 0.0 (TID 0),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.$anonfun$apply$1(FileFormat.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1$$Lambda$3019/0x000070c6ece7f588.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:197)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$2708/0x000070c6ecd8f930.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2709/0x000070c6ecd8fd10.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "25/11/09 14:18:57 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.255.255.254 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.$anonfun$apply$1(FileFormat.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1$$Lambda$3019/0x000070c6ece7f588.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:197)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda$2708/0x000070c6ecd8f930.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2709/0x000070c6ecd8fd10.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "25/11/09 14:18:57 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wystąpił błąd podczas ładowania plików JSON: py4j does not exist in the JVM\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mload_json_files_to_df\u001b[0;34m(spark, file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Wczytanie wszystkich plików JSON z folderu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# df = spark.read.json(f\"{folder_path}/*.json\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/delta/exceptions.py:160\u001b[0m, in \u001b[0;36m_patch_convert_exception.<locals>.convert_delta_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_exception\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_convert_sql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:149\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayIndexOutOfBoundsException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava.time.DateTimeException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DateTimeException(origin\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m file_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_fraud_labels.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m( os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(UNPACKAGED_DATA, folder_path, file_json))\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUNPACKAGED_DATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mload_json_files_to_df\u001b[0;34m(spark, file_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWystąpił błąd podczas ładowania plików JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Zwrócenie pustego DataFrame w przypadku błędu\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1383\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1381\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1386\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOULD_NOT_DATAFRAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1387\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1388\u001b[0m     )\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "folder_path = 'transactions-fraud-datasets'\n",
    "file_json = 'train_fraud_labels.json'\n",
    "print( os.path.join(UNPACKAGED_DATA, folder_path, file_json))\n",
    "df = load_json_files_to_df(spark, os.path.join(UNPACKAGED_DATA, folder_path, file_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_fraud_labels.json', 'mcc_codes.json']\n",
      "/home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets/train_fraud_labels.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 12:10:22 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 9)/ 8]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda/0x00007ad62cf9f9b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d0259c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d003900.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62cfec5c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007ad62cc7e6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "25/11/09 12:10:22 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#146,Executor task launch worker for task 0.0 in stage 3.0 (TID 9),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda/0x00007ad62cf9f9b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d0259c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d003900.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62cfec5c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007ad62cc7e6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "25/11/09 12:10:22 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 9) (10.255.255.254 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:393)\n",
      "\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:106)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$$Lambda/0x00007ad62cf9f9b0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d0259c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:47)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62d003900.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$$Lambda/0x00007ad62cfec5c0.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007ad62cc7e6f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\n",
      "25/11/09 12:10:22 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wystąpił błąd podczas ładowania plików JSON: py4j does not exist in the JVM\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 13\u001b[0m, in \u001b[0;36mload_json_files_to_df\u001b[0;34m(spark, file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Wczytanie wszystkich plików JSON z folderu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# df = spark.read.json(f\"{folder_path}/*.json\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/delta/exceptions.py:160\u001b[0m, in \u001b[0;36m_patch_convert_exception.<locals>.convert_delta_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_exception\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_convert_sql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/delta/exceptions.py:160\u001b[0m, in \u001b[0;36m_patch_convert_exception.<locals>.convert_delta_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_exception\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_convert_sql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:143\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IllegalArgumentException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava.lang.ArithmeticException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArithmeticException(origin\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(UNPACKAGED_DATA, folder_path, file_json)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m( full_path)\n\u001b[0;32m---> 11\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUNPACKAGED_DATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     dataframes[file_json] \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# my_dataframe = load_json_files_to_df(spark, os.join(UNPACKAGED_DATA, folder_path))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# if my_dataframe.count() > 0:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # Utworzenie tymczasowego widoku o nazwie 'my_temp_view'\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# my_dataframe.createOrReplaceTempView(\"my_temp_view\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 18\u001b[0m, in \u001b[0;36mload_json_files_to_df\u001b[0;34m(spark, file_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWystąpił błąd podczas ładowania plików JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Zwrócenie pustego DataFrame w przypadku błędu\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1383\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1381\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1386\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOULD_NOT_DATAFRAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1387\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1388\u001b[0m     )\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Przykład użycia z już otwartą sesją Spark\n",
    "# Zakładając, że masz już aktywną sesję Spark o nazwie 'spark'\n",
    "\n",
    "folder_path = 'transactions-fraud-datasets'\n",
    "dataframes = {}\n",
    "files_json = [f for f in os.listdir(os.path.join(UNPACKAGED_DATA, folder_path)) if f.endswith('.json')]\n",
    "print( files_json)\n",
    "for file_json in files_json:\n",
    "    full_path = os.path.join(UNPACKAGED_DATA, folder_path, file_json)\n",
    "    print( full_path)\n",
    "    df = load_json_files_to_df(spark, os.path.join(UNPACKAGED_DATA, folder_path, file_json))\n",
    "    dataframes[file_json] = df\n",
    "# my_dataframe = load_json_files_to_df(spark, os.join(UNPACKAGED_DATA, folder_path))\n",
    "\n",
    "# if my_dataframe.count() > 0:\n",
    "#     my_dataframe.printSchema()\n",
    "#     my_dataframe.show()\n",
    "\n",
    "# # Utworzenie tymczasowego widoku o nazwie 'my_temp_view'\n",
    "# my_dataframe.createOrReplaceTempView(\"my_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets/train_fraud_labels.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. Wczytanie pliku CSV\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[1;32m      6\u001b[0m     csv_file_path,\n\u001b[1;32m      7\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,       \u001b[38;5;66;03m# Użyj pierwszej linii jako nagłówków kolumn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Spróbuj automatycznie wykryć typy danych (np. Int, String, Date)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# Określ separator (domyślnie przecinek, ale można zmienić na ';', '\\t' itd.)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# Określ kodowanie pliku\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 3. Wyświetlenie pierwszych wierszy i schematu\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Pierwsze 5 wierszy ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# 1. Definicja ścieżki do pliku\n",
    "csv_file_path = \"/home/r/git_projekty/sparkTraining/data/unpackaged/transactions-fraud-datasets/train_fraud_labels.json\"\n",
    "\n",
    "# 2. Wczytanie pliku CSV\n",
    "df = spark.read.csv(\n",
    "    csv_file_path,\n",
    "    header=True,       # Użyj pierwszej linii jako nagłówków kolumn\n",
    "    inferSchema=False,  # Spróbuj automatycznie wykryć typy danych (np. Int, String, Date)\n",
    "    sep=\",\",           # Określ separator (domyślnie przecinek, ale można zmienić na ';', '\\t' itd.)\n",
    "    encoding=\"UTF-8\"   # Określ kodowanie pliku\n",
    ")\n",
    "\n",
    "# 3. Wyświetlenie pierwszych wierszy i schematu\n",
    "print(\"--- Pierwsze 5 wierszy ---\")\n",
    "df.show(5)\n",
    "\n",
    "print(\"--- Schemat (struktura danych) ---\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "def load_json_files_to_df_(spark: SparkSession, folder_path: str, name_patterns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ładuje pliki JSON z podanego folderu do DataFrame, używając istniejącej sesji Spark.\n",
    "    Filtruje pliki, które mają w nazwie wszystkie podane wzorce.\n",
    "\n",
    "    Args:\n",
    "        spark: Aktywna sesja Spark.\n",
    "        folder_path: Ścieżka do folderu zawierającego pliki JSON.\n",
    "        name_patterns: Lista stringów, które muszą znaleźć się w nazwie pliku.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame PySpark z załadowanymi danymi lub pusty DataFrame, jeśli nie znaleziono pasujących plików.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tworzenie wzorca glob do odczytu plików\n",
    "    file_pattern = folder_path + \"/*\" + \"*\".join(name_patterns) + \"*.json\"\n",
    "\n",
    "    try:\n",
    "        # Odczyt plików pasujących do wzorca\n",
    "        df = spark.read.option(\"multiLine\", \"true\").json(file_pattern)\n",
    "\n",
    "        # Sprawdzenie, czy DataFrame jest pusty\n",
    "        if df.rdd.isEmpty():\n",
    "            print(f\"Brak plików pasujących do wzorca '{file_pattern}'. Zwracam pusty DataFrame.\")\n",
    "            return spark.createDataFrame([], \"struct<col:string>\")\n",
    "\n",
    "        # # Dodanie nowej kolumny 'file_name' z pełną ścieżką pliku\n",
    "        # df_with_filename = df.withColumn(\"file_name\", input_file_name())\n",
    "\n",
    "        # Dodanie nowej kolumny 'file_name' z samą nazwą pliku\n",
    "        # regexp_extract(input_file_name(), '([^/]+)$', 1)\n",
    "        # ^ - początek stringa\n",
    "        # [^/] - dowolny znak oprócz ukośnika\n",
    "        # + - jeden lub więcej znaków\n",
    "        # $ - koniec stringa\n",
    "        df_with_filename = df.withColumn(\n",
    "            \"file_name\",\n",
    "            regexp_extract(input_file_name(), '([^/]+)$', 1)\n",
    "        )\n",
    "        print(f\"Pomyślnie załadowano pliki do DataFrame.\")\n",
    "        return df_with_filename\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił błąd podczas ładowania plików: {e}\")\n",
    "        return spark.createDataFrame([], \"struct<col:string>\")\n",
    "\n",
    "# Przykład użycia z już otwartą sesją Spark\n",
    "# Zakładając, że masz już aktywną sesję Spark o nazwie 'spark'\n",
    "# folder_path = \"/path/to/your/json/folder\"\n",
    "# patterns = ['product_status_campaign_id', 'line_item_id']\n",
    "# my_dataframe = load_json_files_to_df(spark, folder_path, patterns)\n",
    "\n",
    "# if my_dataframe.count() > 0:\n",
    "#     my_dataframe.printSchema()\n",
    "#     my_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wystąpił błąd podczas ładowania plików: [PATH_NOT_FOUND] Path does not exist: file:/home/r/pgetl/sparkTraining/extracted_data/*entity=product_status__campaign_id=*__line_item_id=*.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Przykład użycia z już otwartą sesją Spark\n",
    "# Zakładając, że masz już aktywną sesję Spark o nazwie 'spark'\n",
    "folder_path = \"/home/r/pgetl/sparkTraining/extracted_data\"\n",
    "patterns = ['entity=product_status__campaign_id=', '__line_item_id=']\n",
    "# entity=product_status__campaign_id=*__line_item_id=*__page=\n",
    "my_dataframe = load_json_files_to_df(spark, folder_path, patterns)\n",
    "\n",
    "if my_dataframe.count() > 0:\n",
    "    my_dataframe.printSchema()\n",
    "    my_dataframe.show()\n",
    "\n",
    "# Utworzenie tymczasowego widoku o nazwie 'my_temp_view'\n",
    "my_dataframe.createOrReplaceTempView(\"my_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `api_product_id` cannot be resolved. Did you mean one of the following? [`col`].; line 1 pos 36;\n'Sort ['api_product_id ASC NULLS FIRST], true\n+- Project [col#0]\n   +- SubqueryAlias my_temp_view\n      +- View (`my_temp_view`, [col#0])\n         +- LogicalRDD [col#0], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparksql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM my_temp_view order by api_product_id --limit 100\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/sparksql_magic/sparksql.py:40\u001b[0m, in \u001b[0;36mSparkSql.sparksql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive spark session is not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache dataframe with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m load\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlazy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `api_product_id` cannot be resolved. Did you mean one of the following? [`col`].; line 1 pos 36;\n'Sort ['api_product_id ASC NULLS FIRST], true\n+- Project [col#0]\n   +- SubqueryAlias my_temp_view\n      +- View (`my_temp_view`, [col#0])\n         +- LogicalRDD [col#0], false\n"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM my_temp_view order by api_product_id --limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request po product status dla tej kampani dla tej adtroupy i zobaczyc co dostaniemy\n",
    "# URL, z którym będziesz się łączyć\n",
    "API_URL = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Zapytanie GET: Pobieranie danych z API\n",
    "def request_get():\n",
    "    print(\"--- Zapytanie GET ---\")\n",
    "    try:\n",
    "        response = requests.get(API_URL)\n",
    "\n",
    "        # Sprawdzenie, czy zapytanie zakończyło się sukcesem (kod 200)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Pobranie danych jako JSON\n",
    "        posts = response.json()\n",
    "        print(\"Pobrano 100 postów.\")\n",
    "\n",
    "        # Wyświetlenie pierwszego posta\n",
    "        print(\"Pierwszy post:\")\n",
    "        print(json.dumps(posts[0], indent=2))\n",
    "        return posts\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Wystąpił błąd GET: {e}\")\n",
    "\n",
    "# ---\n",
    "\n",
    "# Zapytanie POST: Wysyłanie nowych danych do API\n",
    "def request_post():\n",
    "    print(\"\\n--- Zapytanie POST ---\")\n",
    "    new_post = {\n",
    "        \"title\": \"Tytuł nowego posta\",\n",
    "        \"body\": \"Treść nowego posta\",\n",
    "        \"userId\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        post_response = requests.post(API_URL, json=new_post)\n",
    "        post_response.raise_for_status()\n",
    "\n",
    "        # Otrzymanie odpowiedzi od serwera\n",
    "        created_post = post_response.json()\n",
    "        print(\"Nowy post został utworzony:\")\n",
    "        print(json.dumps(created_post, indent=2))\n",
    "        return created_post\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Wystąpił błąd POST: {e}\")\n",
    "\n",
    "# ---\n",
    "\n",
    "# Zapytanie PUT: Aktualizowanie istniejących danych\n",
    "def request_put():\n",
    "    print(\"\\n--- Zapytanie PUT ---\")\n",
    "    update_url = f\"{API_URL}/1\" # Aktualizujemy post o ID 1\n",
    "    updated_data = {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Zaktualizowany tytuł\",\n",
    "        \"body\": \"Zaktualizowana treść\",\n",
    "        \"userId\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        put_response = requests.put(update_url, json=updated_data)\n",
    "        put_response.raise_for_status()\n",
    "\n",
    "        updated_post = put_response.json()\n",
    "        print(\"Post został zaktualizowany:\")\n",
    "        print(json.dumps(updated_post, indent=2))\n",
    "        return updated_post\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Wystąpił błąd PUT: {e}\")\n",
    "\n",
    "# ---\n",
    "\n",
    "# Zapytanie DELETE: Usuwanie danych\n",
    "# def request_delete():\n",
    "#     print(\"\\n--- Zapytanie DELETE ---\")\n",
    "#     delete_url = f\"{API_URL}/1\" # Usuwamy post o ID 1\n",
    "\n",
    "#     try:\n",
    "#         delete_response = requests.delete(delete_url)\n",
    "#         delete_response.raise_for_status()\n",
    "\n",
    "#         print(\"Post o ID 1 został usunięty.\")\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Wystąpił błąd DELETE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df_with_filename.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file1 = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .load(file1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file1.createOrReplaceTempView(\"data_file1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_file1.write.format(\"parquet\").save(\"data_file1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">retailer</td><td style=\"font-weight: bold\">sub_retailer</td><td style=\"font-weight: bold\">country</td><td style=\"font-weight: bold\">region</td><td style=\"font-weight: bold\">postal_code</td><td style=\"font-weight: bold\">scrape_time</td><td style=\"font-weight: bold\">keyword</td><td style=\"font-weight: bold\">ad_type</td><td style=\"font-weight: bold\">device</td><td style=\"font-weight: bold\">brand_raw</td><td style=\"font-weight: bold\">rank</td><td style=\"font-weight: bold\">product_title_id</td><td style=\"font-weight: bold\">product_id</td><td style=\"font-weight: bold\">price_amount</td><td style=\"font-weight: bold\">rating</td><td style=\"font-weight: bold\">review</td><td style=\"font-weight: bold\">product_url</td><td style=\"font-weight: bold\">image_url</td><td style=\"font-weight: bold\">oos</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">time</td><td style=\"font-weight: bold\">data_source</td><td style=\"font-weight: bold\">grid</td><td style=\"font-weight: bold\">paid</td><td style=\"font-weight: bold\">filename</td><td style=\"font-weight: bold\">product_title</td><td style=\"font-weight: bold\">price_unit</td><td style=\"font-weight: bold\">feature_label</td><td style=\"font-weight: bold\">store_id</td><td style=\"font-weight: bold\">requestor_id</td><td style=\"font-weight: bold\">month</td><td style=\"font-weight: bold\">year</td><td style=\"font-weight: bold\">source_file_name</td><td style=\"font-weight: bold\">source_last_mod_timestamp</td><td style=\"font-weight: bold\">dcom_load_timestamp</td><td style=\"font-weight: bold\">category</td><td style=\"font-weight: bold\">dcom_processing_timestamp</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066616</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>1</td><td>NA</td><td>505105842</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Moist-Shampoo-and-Conditioner-Hair-Set-26-2-fl-oz/505105842?classType=REGULAR&amp;athbdg=L1200</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Moist-Shampoo-and-Conditioner-Hair-Set-26-2-fl-oz_d37f65b2-7e60-41ea-a137-560d3db675bc.566f38993f9a9f17807a5987b8dcdeb6.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Moist Shampoo and Conditioner Dual Pack Hair Set 262 fl oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066666</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>2</td><td>NA</td><td>1480516348</td><td>15.0</td><td>4.6</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Curls-Shampoo-and-Conditioner-Dual-Pack-For-All-Hair-Types-26-2-oz/1480516348?classType=REGULAR&amp;athbdg=L1102</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Curls-Shampoo-and-Conditioner-Dual-Pack-For-All-Hair-Types-26-2-oz_68afe0a7-520d-4089-887c-3abccf5ddea6.2981a321dff289c2c9fef81e1ebf01d4.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Curls Shampoo and Conditioner Dual Pack For All Hair Types 262 oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066689</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>3</td><td>NA</td><td>505105842</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Moist-Shampoo-and-Conditioner-Hair-Set-26-2-fl-oz/505105842?classType=REGULAR&amp;athbdg=L1200</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Moist-Shampoo-and-Conditioner-Hair-Set-26-2-fl-oz_d37f65b2-7e60-41ea-a137-560d3db675bc.566f38993f9a9f17807a5987b8dcdeb6.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Moist Shampoo and Conditioner Dual Pack Hair Set 262 fl oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066706</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>4</td><td>NA</td><td>1480516348</td><td>15.0</td><td>4.6</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Curls-Shampoo-and-Conditioner-Dual-Pack-For-All-Hair-Types-26-2-oz/1480516348?classType=REGULAR&amp;athbdg=L1102</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Curls-Shampoo-and-Conditioner-Dual-Pack-For-All-Hair-Types-26-2-oz_68afe0a7-520d-4089-887c-3abccf5ddea6.2981a321dff289c2c9fef81e1ebf01d4.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Curls Shampoo and Conditioner Dual Pack For All Hair Types 262 oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066723</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>5</td><td>NA</td><td>853590359</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Volume-Shampoo-and-Conditioner-Hair-Set-All-Hair-Types-26-2-fl-oz/853590359?classType=VARIANT</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Volume-Shampoo-and-Conditioner-Hair-Set-All-Hair-Types-26-2-fl-oz_3924c90f-46a5-4ad1-b4c7-a6e8181ed26b.0689277e7ce947ebe3647b3ba9e9c07e.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Volume Shampoo and Conditioner Dual Pack Hair Set All Hair Types 262 fl oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066739</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>6</td><td>NA</td><td>5052941127</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Repairer-Dual-Pack-for-All-Hair-Types-26-2-fl-oz-Shampoo-and-Conditioner/5052941127?classType=REGULAR&amp;athbdg=L1600</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Repairer-Dual-Pack-for-All-Hair-Types-26-2-fl-oz-Shampoo-and-Conditioner_738728c6-0f06-4f94-993e-6418fd531974.947cc2406a06fc0b89516f31e03d28e3.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Repairer Dual Pack for All Hair Types 262 fl oz Shampoo and Conditioner</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066755</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>7</td><td>NA</td><td>40694186</td><td>8.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Total-Miracle-Shampoo-Paraben-Free-for-All-Hair-Types-26-2-fl-oz/40694186?classType=REGULAR&amp;athbdg=L1102</td><td>https://i5.walmartimages.com/seo/Aussie-Total-Miracle-Shampoo-Paraben-Free-for-All-Hair-Types-26-2-fl-oz_05648013-363e-4ff5-9e38-cbe518284691.88475fa917d8ccb8d133a612b35913b9.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Total Miracle Shampoo Paraben Free for All Hair Types 262 fl oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066775</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>8</td><td>NA</td><td>224262046</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Total-Miracle-Apricot-Macadamia-Oil-Paraben-Free-Shampoo-and-Conditioner-Twin-Pack/224262046?classType=REGULAR&amp;athbdg=L1102</td><td>https://i5.walmartimages.com/seo/Aussie-Total-Miracle-Apricot-Macadamia-Oil-Paraben-Free-Shampoo-and-Conditioner-Twin-Pack_b2467961-20c2-439e-bf77-5c9f191df5dd.3872d93fcf066cdcd6e649f02b3a4308.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Total Miracle Apricot Macadamia Oil Paraben Free Shampoo and Conditioner Dual Pack</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066790</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>9</td><td>NA</td><td>5052941127</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Repairer-Dual-Pack-for-All-Hair-Types-26-2-fl-oz-Shampoo-and-Conditioner/5052941127?classType=REGULAR&amp;athbdg=L1600</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Repairer-Dual-Pack-for-All-Hair-Types-26-2-fl-oz-Shampoo-and-Conditioner_738728c6-0f06-4f94-993e-6418fd531974.947cc2406a06fc0b89516f31e03d28e3.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Repairer Dual Pack for All Hair Types 262 fl oz Shampoo and Conditioner</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066806</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>10</td><td>NA</td><td>270111177</td><td>8.0</td><td>4.6</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Moist-Conditioner-with-Avocado-Paraben-Free-For-Dry-Hair-Types-26-2-oz/270111177?classType=REGULAR&amp;athbdg=L1102</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Moist-Conditioner-with-Avocado-Paraben-Free-For-Dry-Hair-Types-26-2-oz_808d9a9b-4e17-46f9-89d7-355488f6f4f7.c55b349031fcfa23b71441cf53b3f6ba.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Moist Conditioner with Avocado Paraben Free For Dry Hair Types 262 oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066821</td><td>aussie shampoo and conditioner</td><td>sp</td><td>mobile-app</td><td>null</td><td>11</td><td>NA</td><td>16417865851</td><td>25.0</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Total-Miracle-7-Benefit-Shampoo-And-Conditioner-Set-With-Apricot-Oil-12-1-fl-oz-each/16417865851?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Total-Miracle-7-Benefit-Shampoo-And-Conditioner-Set-With-Apricot-Oil-12-1-fl-oz-each_eae9de7a-6aea-4415-88c4-677ba5676549.9e972d0126053c459cc09d5664937305.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>True</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Total Miracle 7 Benefit Shampoo And Conditioner Set With Apricot Oil 121 fl oz each</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066837</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>12</td><td>NA</td><td>853590359</td><td>15.0</td><td>4.7</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Volume-Shampoo-and-Conditioner-Hair-Set-All-Hair-Types-26-2-fl-oz/853590359?classType=VARIANT</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Volume-Shampoo-and-Conditioner-Hair-Set-All-Hair-Types-26-2-fl-oz_3924c90f-46a5-4ad1-b4c7-a6e8181ed26b.0689277e7ce947ebe3647b3ba9e9c07e.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Volume Shampoo and Conditioner Dual Pack Hair Set All Hair Types 262 fl oz</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066855</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>13</td><td>NA</td><td>5333232478</td><td>20.9</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Curls-Shampoo-Conditioner-Deep-Conditioner-Set-with-Coconut-Jojoba-Oil-For-All-Hair-Types-Paraben-Free/5333232478?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Curls-Shampoo-Conditioner-Deep-Conditioner-Set-with-Coconut-Jojoba-Oil-For-All-Hair-Types-Paraben-Free_fff3d841-3fdb-4633-9865-b322dc83f7e8.df2552946ce51bd5e2bdd63931ab5757.png?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Curls Shampoo Conditioner Deep Conditioner Set with Coconut Jojoba Oil For All Hair Types Paraben Free</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066870</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>14</td><td>NA</td><td>5329180598</td><td>14.9</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Kids-Shampoo-Conditioner-Detangler-Set-Moisturizes-Hair-Sulfate-Free-For-All-Hair-Types/5329180598?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Kids-Shampoo-Conditioner-Detangler-Set-Moisturizes-Hair-Sulfate-Free-For-All-Hair-Types_8e6a6a8e-a3d3-476f-a405-a6b9c8f0838d.a02684353bface2f8015f0c4a498c0e8.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Kids Shampoo Conditioner Detangler Set Moisturizes Hair Sulfate Free For All Hair Types</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066886</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>15</td><td>NA</td><td>16476008530</td><td>14.9</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Moisture-Everything-Shower-Kit/16476008530?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Moisture-Everything-Shower-Kit_7917b2ec-030e-4b3a-b9b2-b813a9c2caba.b78907790cd8a12aef55bbb7a435547a.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Moisture Everything Shower Kit</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066904</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>16</td><td>NA</td><td>3383221200</td><td>28.5</td><td>4.6</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Multi-Miracle-Shampoo-and-Conditioner-Pom-and-Shea-Butter-33-Fl-Oz-2-Pack/3383221200?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Multi-Miracle-Shampoo-and-Conditioner-Pom-and-Shea-Butter-33-Fl-Oz-2-Pack_02aee909-a75d-4396-b71f-cefe0089685f.dbcfa142091d0d31f65af79c49f2882d.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Multi Miracle Shampoo and Conditioner Pom and Shea Butter 33 Fl Oz 2 Pack</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066919</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>17</td><td>NA</td><td>16495664570</td><td>19.9</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Curly-Girl-Starter-Kit/16495664570?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Curly-Girl-Starter-Kit_2169965d-7d10-4747-9c5c-880da2c3ddbd.2d23862aa7af541dffbc3b36356b45a5.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Curly Girl Starter Kit</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066935</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>18</td><td>NA</td><td>1698268744</td><td>20.4</td><td>4.6</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Miracle-Moist-Shampoo-Conditioner-Deep-Conditioner-Set-Paraben-Free-for-Dry-Hair-Types/1698268744?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Miracle-Moist-Shampoo-Conditioner-Deep-Conditioner-Set-Paraben-Free-for-Dry-Hair-Types_c5389e49-21f8-4d62-a174-5d6dcb70d26b.2e10033d115bb21bbd5a61751889af22.png?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Miracle Moist Shampoo Conditioner Deep Conditioner Set Paraben Free for Dry Hair Types</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066951</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>19</td><td>NA</td><td>5812442337</td><td>24.7</td><td>3.0</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Multi-Miracle-Shampoo-and-Conditioner-Moisture-Softness-Shine-33-8-Fl-Oz-2-Pk/5812442337?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Multi-Miracle-Shampoo-and-Conditioner-Moisture-Softness-Shine-33-8-Fl-Oz-2-Pk_8b044a35-c356-4828-984a-7f6d46f045a2.babb7eaaa2934b1135f33d614b256f1b.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Multi Miracle Shampoo and Conditioner Moisture Softness Shine 338 Fl Oz 2 Pk</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr><tr><td>Walmart</td><td>NA</td><td>US</td><td>US</td><td>95829</td><td>2025-08-08 11:59:16.066966</td><td>aussie shampoo and conditioner</td><td>organic</td><td>mobile-app</td><td>null</td><td>20</td><td>NA</td><td>16417865851</td><td>25.0</td><td>null</td><td>NA</td><td>https://www.walmart.com/ip/Aussie-Total-Miracle-7-Benefit-Shampoo-And-Conditioner-Set-With-Apricot-Oil-12-1-fl-oz-each/16417865851?classType=REGULAR</td><td>https://i5.walmartimages.com/seo/Aussie-Total-Miracle-7-Benefit-Shampoo-And-Conditioner-Set-With-Apricot-Oil-12-1-fl-oz-each_eae9de7a-6aea-4415-88c4-677ba5676549.9e972d0126053c459cc09d5664937305.jpeg?odnHeight=1500&amp;odnWidth=1500&amp;odnBg=ffffff</td><td>null</td><td>2025-08-08</td><td>11:59:16 AM</td><td>xbyte</td><td>True</td><td>False</td><td>ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>Aussie Total Miracle 7 Benefit Shampoo And Conditioner Set With Apricot Oil 121 fl oz each</td><td>USD</td><td>null</td><td>none</td><td>scheduler_on_VM</td><td>8</td><td>2025</td><td>abfss://raw@sa02flexflowdcom01prod.dfs.core.windows.net/schedule/xbyte/walmart/scheduler_on_VM/3eec335c-11e6-4497-8fbe-754d0f84d148/keyword_search/2025/08/ab219ef7-b37a-470b-ba1a-db8666d221a7_20250808_11-59-17.json</td><td>2025-08-08 11:59:18</td><td>2025-08-08 12:07:47.817000</td><td>null</td><td>2025-08-08 17:36:25.721487</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM data_file1 limit 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kursy_walut_xml = \"https://static.nbp.pl/dane/kursy/xml/a045z240304.xml\"  # https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2024.csv\n",
    "# kursy_walut_csv = {\n",
    "#     \"2020\": \"https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2020.csv\",\n",
    "#     \"2021\": \"https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2021.csv\",\n",
    "#     \"2022\": \"https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2022.csv\",\n",
    "#     \"2023\": \"https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2023.csv\",\n",
    "#     \"2024\": \"https://static.nbp.pl/dane/kursy/Archiwum/archiwum_tab_a_2024.csv\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nic():\n",
    "    # url = kursy_walut_csv[\"2024\"]\n",
    "    # s = requests.get(url).content\n",
    "    # # pd_kursy = pd.read_csv(url, encoding='cp1250'   )\n",
    "    # kursy = pd.read_csv(url, sep=\";\", encoding=\"cp1250\", index_col=\"data\")\n",
    "    # pd_kursy = pd.read_csv(\n",
    "    #     kursy_walut_csv[\"2024\"], sep=\";\", encoding=\"cp1250\", index_col=\"data\"\n",
    "    # )\n",
    "    # unzip_files = [\n",
    "    #     \"2015.zip\",\n",
    "    #     \"2016.zip\",\n",
    "    #     \"2017.zip\",\n",
    "    #     \"2019.zip\",\n",
    "    #     \"detroit_911_calls.zip\",\n",
    "    #     \"malaysia_covid_cases.zip\",\n",
    "    # ]\n",
    "    # state_path = 'DATA/MATURY'\n",
    "    # unzip_files2 = [ item for item in os.listdir(state_path) if item.endswith('.zip')]\n",
    "    # print( unzip_files2 )\n",
    "    # #################\n",
    "    # def unzip_file(file_to_unzip):\n",
    "    #     print(file_to_unzip.split(\".\"))\n",
    "    #     with zipfile.ZipFile(file_to_unzip, \"r\") as file:\n",
    "    #         file.extractall(file_to_unzip.split(\".\")[0])\n",
    "    # ###################\n",
    "    # # for item in unzip_files2:\n",
    "    # #     item = os.path.join(state_path, item )\n",
    "    # #     print( item )\n",
    "    # #     unzip_file( item )\n",
    "\n",
    "    # ###################\n",
    "    # files_to_proceed = ['marki.txt','perfumy.txt','sklad.txt']\n",
    "    # paths_to_proceed = ['DATA/MATURY/informatyka-2019-maj-matura-rozszerzona-zalaczniki/Dane_PR']\n",
    "    # ###################\n",
    "    # marki =  os.path.join(paths_to_proceed[0], files_to_proceed[0])\n",
    "    # perfumy =  os.path.join(paths_to_proceed[0], files_to_proceed[1])\n",
    "    # sklad =  os.path.join(paths_to_proceed[0], files_to_proceed[2])\n",
    "    # print( marki )\n",
    "    # print( perfumy )\n",
    "    # print( sklad )\n",
    "    # data_marki = (\n",
    "    #     spark.read.format(\"csv\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .option(\"header\", \"true\")\n",
    "    #     .option(\"sep\", \"\\t\")\n",
    "    #     .load(marki)\n",
    "    # )\n",
    "    # data_perfumy = (\n",
    "    #     spark.read.format(\"csv\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .option(\"header\", \"true\")\n",
    "    #     .option(\"sep\", \"\\t\")\n",
    "    #     .load(perfumy)\n",
    "    # )\n",
    "    # data_sklad = (\n",
    "    #     spark.read.format(\"csv\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .option(\"header\", \"true\")\n",
    "    #     .option(\"sep\", \"\\t\")\n",
    "    #     .load(sklad)\n",
    "    # )\n",
    "    # # data_marki = (\n",
    "    # #     spark.read.text(file1)\n",
    "    # # )\n",
    "    # ###################\n",
    "    # data_marki.show(5)\n",
    "    # ###################\n",
    "    # data_marki.createOrReplaceTempView(\"marki\")\n",
    "    # data_perfumy.createOrReplaceTempView(\"perfumy\")\n",
    "    # data_sklad.createOrReplaceTempView(\"sklad\")\n",
    "    # spark.sql(\"select * from marki\").show(5)\n",
    "    # spark.sql(\"select * from perfumy\").show(5)\n",
    "    # spark.sql(\"select * from sklad\").show(5)\n",
    "    # ###################\n",
    "    # #Podaj listę wszystkich nazw perfum, których jednym ze składników jest „absolut jasminu”.\n",
    "    # spark.sql('select * from perfumy where id_perfum in  (select id_perfum from sklad where nazwa_skladnika = \"absolut jasminu\")').show()\n",
    "    # ###################\n",
    "    # # Podaj  listę  różnych  rodzin  zapachów.  Dla  każdej  rodziny  podaj  jej  nazwę,  cenę  najtańszych\n",
    "    # # perfum z tej rodziny i ich nazwę.  Więcej arkuszy znajdziesz na stronie: arkusze.pl\n",
    "    # spark.sql('select rodzina_zapachow , count(*) from perfumy group by rodzina_zapachow' ).show()\n",
    "    # spark.sql('select rodzina_zapachow , min(cena) as minimum from perfumy group by rodzina_zapachow order by minimum ' ).show()\n",
    "    # ###################\n",
    "    # marki = (\n",
    "    #     spark\n",
    "    #     .table(\"marki\")\n",
    "    # )\n",
    "    # perfumy = (\n",
    "    #     spark\n",
    "    #     .table(\"perfumy\")\n",
    "    # )\n",
    "    # sklad = (\n",
    "    #     spark\n",
    "    #     .table(\"sklad\")\n",
    "    # )\n",
    "    # marki.show(5)\n",
    "    # perfumy.show(5)\n",
    "    # sklad.show(5)\n",
    "    # sklad_absolut = (\n",
    "    #     spark\n",
    "    #     .table(\"sklad\")\n",
    "    #     .select(\"id_perfum\")\n",
    "    #     .where(\"nazwa_skladnika = 'absolut jasminu'\")\n",
    "    #     .orderBy(\"id_perfum\", ascending=False)\n",
    "    # )\n",
    "    # sklad_absolut.show(5)\n",
    "    # sklad_absolut.collect()\n",
    "    # print( sklad_absolut)\n",
    "    # perfumy_absolut = perfumy.join(sklad_absolut, on='id_perfum', how='inner')\n",
    "    # perfumy_absolut.show()\n",
    "    # ###################\n",
    "    # data = (\n",
    "    #     spark.read.format(\"csv\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .option(\"header\", \"true\")\n",
    "    #     .load(\"Wyniki.txt\")\n",
    "    # )\n",
    "    # # df = spark.read.csv(\"Wyniki.txt\")\n",
    "    # ###################\n",
    "    # data.printSchema()\n",
    "    # #####################\n",
    "    # data2.createOrReplaceTempView(\"data\")\n",
    "    # spark.sql(\n",
    "    #     \"\"\"\n",
    "    #         SELECT *\n",
    "    #         FROM data\n",
    "    #         WHERE Punkty > 17\n",
    "    #         \"\"\"\n",
    "    # ).show()\n",
    "    # #####################\n",
    "    # df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"orders/2019.csv\")\n",
    "    # #####################\n",
    "    # orderSchema = StructType(\n",
    "    #     [\n",
    "    #         StructField(\"SalesOrderNumber\", StringType()),\n",
    "    #         StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    #         StructField(\"OrderDate\", DateType()),\n",
    "    #         StructField(\"CustomerName\", StringType()),\n",
    "    #         StructField(\"Email\", StringType()),\n",
    "    #         StructField(\"Item\", StringType()),\n",
    "    #         StructField(\"Quantity\", IntegerType()),\n",
    "    #         StructField(\"UnitPrice\", FloatType()),\n",
    "    #         StructField(\"Tax\", FloatType()),\n",
    "    #     ]\n",
    "    # )\n",
    "    # #####################\n",
    "    # dfx = spark.read.load(\n",
    "    #     \"/home/radek/git_projekty/sparkTraining/datafabrictraining/Files/partitioned_data\",\n",
    "    #     format=\"parquet\",\n",
    "    # )\n",
    "    # #####################\n",
    "    # ## Create Year and Month columns\n",
    "    # transformed_df = dfall.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\n",
    "    #     \"Month\", month(col(\"OrderDate\"))\n",
    "    # )\n",
    "\n",
    "    # # Create the new FirstName and LastName fields\n",
    "    # transformed_df = transformed_df.withColumn(\n",
    "    #     \"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)\n",
    "    # ).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
    "\n",
    "    # # Filter and reorder columns\n",
    "    # transformed_df = transformed_df[\n",
    "    #     \"SalesOrderNumber\",\n",
    "    #     \"SalesOrderLineNumber\",\n",
    "    #     \"OrderDate\",\n",
    "    #     \"Year\",\n",
    "    #     \"Month\",\n",
    "    #     \"FirstName\",\n",
    "    #     \"LastName\",\n",
    "    #     \"Email\",\n",
    "    #     \"Item\",\n",
    "    #     \"Quantity\",\n",
    "    #     \"UnitPrice\",\n",
    "    #     \"Tax\",\n",
    "    # ]\n",
    "\n",
    "    # # Display the first five orders\n",
    "    # display(transformed_df.limit(5))\n",
    "    # transformed_df.show(5)\n",
    "    # #####################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the transformed data\n",
    "Add a new cell with the following code to save the transformed dataframe in Parquet format (Overwriting the data if it already exists):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data in partitioned files\n",
    "# Add a new cell with the following code; which saves the dataframe, partitioning the data by Year and Month:\n",
    "\n",
    "````\n",
    "orders_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(\"Files/partitioned_data\")\n",
    "print (\"Transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext sparksql_magic\n",
    "# %config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame called 'df'\n",
    "dfall.createOrReplaceTempView(\"my_temp_view\")\n",
    "transformed_df.createOrReplaceTempView(\"my_temp_view2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "|SalesOrderNumber|SalesOrderLineNumber| OrderDate|  CustomerName|               Email|                Item|Quantity|UnitPrice|     Tax|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "|         SO49171|                   1|2021-01-01| Mariah Foster|mariah21@adventur...|  Road-250 Black, 48|       1|2181.5625| 174.525|\n",
      "|         SO49172|                   1|2021-01-01|  Brian Howard|brian23@adventure...|    Road-250 Red, 44|       1|  2443.35| 195.468|\n",
      "|         SO49173|                   1|2021-01-01| Linda Alvarez|linda19@adventure...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49174|                   1|2021-01-01|Gina Hernandez|gina4@adventure-w...|Mountain-200 Silv...|       1|2071.4197|165.7136|\n",
      "|         SO49178|                   1|2021-01-01|     Beth Ruiz|beth4@adventure-w...|Road-550-W Yellow...|       1|1000.4375|  80.035|\n",
      "+----------------+--------------------+----------+--------------+--------------------+--------------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now you can query the view using Spark SQL\n",
    "spark.sql(\"SELECT * FROM my_temp_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">SalesOrderNumber</td><td style=\"font-weight: bold\">SalesOrderLineNumber</td><td style=\"font-weight: bold\">OrderDate</td><td style=\"font-weight: bold\">CustomerName</td><td style=\"font-weight: bold\">Email</td><td style=\"font-weight: bold\">Item</td><td style=\"font-weight: bold\">Quantity</td><td style=\"font-weight: bold\">UnitPrice</td><td style=\"font-weight: bold\">Tax</td></tr><tr><td>SO49171</td><td>1</td><td>2021-01-01</td><td>Mariah Foster</td><td>mariah21@adventure-works.com</td><td>Road-250 Black, 48</td><td>1</td><td>2181.5625</td><td>174.52499389648438</td></tr><tr><td>SO49172</td><td>1</td><td>2021-01-01</td><td>Brian Howard</td><td>brian23@adventure-works.com</td><td>Road-250 Red, 44</td><td>1</td><td>2443.35009765625</td><td>195.46800231933594</td></tr><tr><td>SO49173</td><td>1</td><td>2021-01-01</td><td>Linda Alvarez</td><td>linda19@adventure-works.com</td><td>Mountain-200 Silver, 38</td><td>1</td><td>2071.419677734375</td><td>165.71359252929688</td></tr><tr><td>SO49174</td><td>1</td><td>2021-01-01</td><td>Gina Hernandez</td><td>gina4@adventure-works.com</td><td>Mountain-200 Silver, 42</td><td>1</td><td>2071.419677734375</td><td>165.71359252929688</td></tr><tr><td>SO49178</td><td>1</td><td>2021-01-01</td><td>Beth Ruiz</td><td>beth4@adventure-works.com</td><td>Road-550-W Yellow, 44</td><td>1</td><td>1000.4375</td><td>80.03500366210938</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM my_temp_view LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 09:07:27 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier\n",
      "24/03/26 09:07:27 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|OrderYear|        GrossRevenue|\n",
      "+---------+--------------------+\n",
      "|     2019|   4172169.969970703|\n",
      "|     2020|   6882259.268127441|\n",
      "|     2021|1.1547835291696548E7|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\n",
    "#                 SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\n",
    "#             FROM my_temp_view2 \\\n",
    "#             GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\n",
    "#             ORDER BY OrderYear\"\n",
    "# df_spark = spark.sql(sqlQuery)\n",
    "# df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfa = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"products/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ProductID: string, ProductName: string, Category: string, ListPrice: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+---------+\n",
      "|ProductID|         ProductName|      Category|ListPrice|\n",
      "+---------+--------------------+--------------+---------+\n",
      "|      771|Mountain-100 Silv...|Mountain Bikes|3399.9900|\n",
      "|      772|Mountain-100 Silv...|Mountain Bikes|3399.9900|\n",
      "|      773|Mountain-100 Silv...|Mountain Bikes|3399.9900|\n",
      "|      774|Mountain-100 Silv...|Mountain Bikes|3399.9900|\n",
      "|      775|Mountain-100 Blac...|Mountain Bikes|3374.9900|\n",
      "+---------+--------------------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display(dfa)\n",
    "# dfa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfa.write.format(\"delta\").saveAsTable(\"dfa\")\n",
    "# dfa.write.format(\"delta\").saveAsTable(\"dfa\",path=\"products\")\n",
    "# dfa.write.format(\"delta\").saveAsTable(\"managed_products\")\n",
    "# dfa.createOrReplaceTempView(\"dfa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">col_name</td><td style=\"font-weight: bold\">data_type</td><td style=\"font-weight: bold\">comment</td></tr><tr><td>ProductID</td><td>string</td><td>null</td></tr><tr><td>ProductName</td><td>string</td><td>null</td></tr><tr><td>Category</td><td>string</td><td>null</td></tr><tr><td>ListPrice</td><td>string</td><td>null</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%sparksql\n",
    "# DESCRIBE FORMATTED dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sparksql\n",
    "# UPDATE dfa\n",
    "# SET  ListPrice = ListPrice * 0.9\n",
    "# WHERE Category = 'Mountain Bikes';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext Kqlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdimCustomer_silver.createOrReplaceTempView(\"dfdimCustomer_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 09:15:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Christy Zhu, christy12@adventure-works.com\n",
      " Schema: CustomerName, Email\n",
      "Expected: CustomerName but found: Christy Zhu\n",
      "CSV file: file:///home/radek/git_projekty/sparkTraining/2019.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">CustomerName</td><td style=\"font-weight: bold\">Email</td><td style=\"font-weight: bold\">First</td><td style=\"font-weight: bold\">Last</td></tr><tr><td>Jonathon Gutierrez</td><td>jonathon8@adventure-works.com</td><td>Jonathon</td><td>Gutierrez</td></tr><tr><td>Blake Butler</td><td>blake62@adventure-works.com</td><td>Blake</td><td>Butler</td></tr><tr><td>Melissa Perry</td><td>melissa2@adventure-works.com</td><td>Melissa</td><td>Perry</td></tr><tr><td>Hailey James</td><td>hailey17@adventure-works.com</td><td>Hailey</td><td>James</td></tr><tr><td>Jasmine West</td><td>jasmine37@adventure-works.com</td><td>Jasmine</td><td>West</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from dfdimCustomer_silver limit 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ProductID: string, ProductName: string, Category: string, ListPrice: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %pyspark\n",
    "df7 = spark.read.load(\"products.csv\", format=\"csv\", header=True)\n",
    "display(df7.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">ProdID</td><td style=\"font-weight: bold\">ProdName</td><td style=\"font-weight: bold\">Category</td><td style=\"font-weight: bold\">ListPrice</td></tr><tr><td>null</td><td>ProductName</td><td>Category</td><td>null</td></tr><tr><td>771</td><td>Mountain-100 Silver, 38</td><td>Mountain Bikes</td><td>3399.989990234375</td></tr><tr><td>772</td><td>Mountain-100 Silver, 42</td><td>Mountain Bikes</td><td>3399.989990234375</td></tr><tr><td>773</td><td>Mountain-100 Silver, 44</td><td>Mountain Bikes</td><td>3399.989990234375</td></tr><tr><td>774</td><td>Mountain-100 Silver, 48</td><td>Mountain Bikes</td><td>3399.989990234375</td></tr><tr><td>775</td><td>Mountain-100 Black, 38</td><td>Mountain Bikes</td><td>3374.989990234375</td></tr><tr><td>776</td><td>Mountain-100 Black, 42</td><td>Mountain Bikes</td><td>3374.989990234375</td></tr><tr><td>777</td><td>Mountain-100 Black, 44</td><td>Mountain Bikes</td><td>3374.989990234375</td></tr><tr><td>778</td><td>Mountain-100 Black, 48</td><td>Mountain Bikes</td><td>3374.989990234375</td></tr><tr><td>779</td><td>Mountain-200 Silver, 38</td><td>Mountain Bikes</td><td>2319.989990234375</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from products limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
