.create table sales (
     SalesOrderNumber: string,
     SalesOrderLineItem: int,
     OrderDate: datetime,
     CustomerName: string,
     EmailAddress: string,
     Item: string,
     Quantity: int,
     UnitPrice: real,
     TaxAmount: real)


.ingest into table sales 'https://<StorageAccountName>.blob.core.windows.net/container/<TableName>.csv' 
 with (ignoreFirstRecord = true)


 sales

 sales
| take 5

sales
| take 5


sales
| where OrderDate between (now(-1d) .. now())

sales
| where Item contains 'Mountain-100'
| sort by OrderDate desc

sales
| summarize ItemsSold= sum(Quantity) by Item



# Read a table into a DataFrame
df = spark.table("your_table_name")

# Write the DataFrame to a CSV file
df.write.csv("your_file_name.csv")

//***********************************************************************************************************// Here are two articles to help you get started with KQL:// KQL reference guide - https://aka.ms/KQLguide// SQL - KQL conversions - https://aka.ms/sqlcheatsheet//***********************************************************************************************************// Use "take" to view a sample number of records in the table and check the data.// Automotive// | take 10// See how many records are in the table.// Automotive// | count// This query returns the number of ingestions per hour in the given table.// Automotive// | summarize IngestionCount = count() by bin(ingestion_time(), 1h)// Automotive// | where passenger_count > 4// | take 10// Automotive// | where passenger_count > 5// | take 10// Automotive// | count// explain SELECT MAX(passenger_count) as maks FROM Automotive // Automotive// | take 5Automotive| take 20| project dropoff_datetime,  pickup_datetime| extend durration = dropoff_datetime - pickup_datetimeAutomotive | take 5Automotive| take 10 | extend DatePart = format_datetime(pickup_datetime, 'yyyy-MM-dd')| extend TimePart = format_datetime(pickup_datetime, 'HH:mm:ss')| extend DatePartdrop = format_datetime(dropoff_datetime, 'yyyy-MM-dd')| extend TimePartdrop = format_datetime(dropoff_datetime, 'HH:mm:ss')| project DatePart, TimePart, DatePartdrop, TimePartdrop| getschema Automotive | take 5| getschema | summarize TotalRecords = count() by ColumnType| render piechart// logsRaw// | where Component in (// 'INGESTOR_EXECUTER', // 'INGESTOR_GATEWAY', // 'INTEGRATIONDATABASE',// 'INTEGRATIONSERVICEFLOWS', // 'INTEGRATIONSERVICETRACE')// .create table ingestionLogs (// Timestamp: datetime, // Source: string,// Node: string, // Level: string, // Component: string, // ClientRequestId: string, // Message: string, // Properties: dynamic)// .create function ingestionComponents(){// logsRaw// | where Component has_any ('INGESTOR_EXECUTER', 'INGESTOR_GATEWAY', 'INTEGRATIONDATABASE','INTEGRATIONSERVICEFLOWS', 'INTEGRATIONSERVICETRACE', 'DOWNLOADER')// }
KQL

// // SELECT COUNT() FROM logsRaw// // explain SELECT MAX(Timestamp) AS MaxTimestamp FROM logsRaw WHERE Level='Error'// logsRaw//   | where Level=="Error"//   | take 10// logsRaw// | summarize count() // or: count// logsRaw// | summarize min(Timestamp), max(Timestamp)// logsRaw// | where Component == "DOWNLOADER"// | take 10// logsRaw// | where Component == "DOWNLOADER"// | take 100// | extend originalSize=Properties.OriginalSize, compressedSize=Properties.compressedSize// logsRaw// | extend originalSize=Properties.OriginalSize, compressedSize=Properties.compressedSize// | getschema // logsRaw// | where Timestamp between (datetime(2014-03-08 01:00).. datetime(2014-03-08 10:00))// | project Timestamp, ClientRequestId, Level, Message// | take 10// logsRaw// | project Timestamp, ClientRequestId, Level, Message// | take 10// logsRaw// | where Component == 'INGESTOR_EXECUTER'// | extend rowCount=toint(Properties.rowCount)// | where isnotempty(rowCount)// | sort by rowCount// | top 10 by rowCount desc// logsRaw// | where Component == 'INGESTOR_EXECUTER'// | extend rowCount=toint(Properties.rowCount), fileFormat=tostring(Properties.format) // | project Timestamp, fileFormat, rowCount, ClientRequestId, Component, Level, Message// | take 10// logsRaw// | summarize count() by Component// logsRaw// | where Message has "ingestion"// | summarize count() by Level// logsRaw// | summarize count() by Level// | render piechart// logsRaw// | summarize count() by bin(Timestamp, 30m)// | render timechart// logsRaw// | where Component in (// 'INGESTOR_EXECUTER', // 'INGESTOR_GATEWAY', // 'INTEGRATIONDATABASE',// 'INTEGRATIONSERVICEFLOWS', // 'INTEGRATIONSERVICETRACE')// .create table ingestionLogs (// Timestamp: datetime, // Source: string,// Node: string, // Level: string, // Component: string, // ClientRequestId: string, // Message: string, // Properties: dynamic)// .create function ingestionComponents(){// logsRaw// | where Component has_any ('INGESTOR_EXECUTER', 'INGESTOR_GATEWAY', 'INTEGRATIONDATABASE','INTEGRATIONSERVICEFLOWS', 'INTEGRATIONSERVICETRACE', 'DOWNLOADER')// }

// Note: execute the below commands one after another => Using operationId(output of each command), //check the status and execute a new command only after the previous one is completed.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/00/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv',           creationTime='2014-03-08T00:00:00Z');.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/01/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv',         creationTime='2014-03-08T01:00:00Z');.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/02/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv',         creationTime='2014-03-08T02:00:00Z');.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/03/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv',         creationTime='2014-03-08T03:00:00Z');.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/04/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv',         creationTime='2014-03-08T04:00:00Z');

// 1. Using pipe: Count how many tables are in the database-in-scope:.show tables| count// 2. Using semicolon: Count how many tables are in the database-in-scope:.show tables;$command_results| count// 3. Using semicolon, and including a let statement:.show tables;let useless=(n:string){strcat(n,'-','useless')};$command_results | extend LastColumn=useless(TableName).show queries | where StartedOn >ago(7d)| summarize count() by User

// 1. Using pipe: Count how many tables are in the database-in-scope:.show tables| count// 2. Using semicolon: Count how many tables are in the database-in-scope:.show tables;$command_results| count// 3. Using semicolon, and including a let statement:.show tables;let useless=(n:string){strcat(n,'-','useless')};$command_results | extend LastColumn=useless(TableName).show queries | where StartedOn >ago(7d)| summarize count() by User

.show commands
| where StartedOn >ago(4h)
| summarize count() by User

.show commands
| where StartedOn >ago(4h)
| summarize count() by User

let LogType = 'Warning';let TimeBucket = 1m;logsRaw| summarize count() by Level = LogType, bin(Timestamp,TimeBucket)| render timechart

search  "Exception=System.Timeout" | count

logsRaw 
| where Component == "INGESTOR_EXECUTER" 
//| parse-kv Properties as (size: int, format: string, rowCount: int, cpuTime: string , duration: string) //bug: cpuTime, duration truncated
| take 20
| evaluate bag_unpack(Properties)

ingestionLogs 
| where Component == "INGESTOR_EXECUTER" 
| take 20 
| parse-kv Message as (IngestionCompletionEvent:string, path:string) with (pair_delimiter=' file', kv_delimiter=':')

logsRaw
| where Component=='INGESTOR_EXECUTER' and Node =='Engine000000000378'
| extend size=tolong(Properties.size)
| summarize avg(size) by bin(Timestamp, 1h), Node
| render timechart 

logsRaw
| where Component=='INGESTOR_EXECUTER' and Node =='Engine000000000378'
| extend size=tolong(Properties.size)
| make-series avg(size) default=0 on Timestamp step 1h by Node
| render timechart

ingestionLogs
| extend size=tolong(Properties.size)
| make-series avg(size) default=0 on Timestamp step 10m
| extend anom=series_decompose_anomalies(avg_size)
| render anomalychart  with (anomalycolumns=anom)


ingestionLogs
| extend size=tolong(Properties.size)
| make-series avg(size) default=0 on Timestamp step 10m
| extend anom=series_decompose_anomalies(avg_size, 0.5)
| mv-expand Timestamp, avg_size, anom
| where anom <> 0