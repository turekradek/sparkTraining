{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2831/2576859890.py:12: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n",
    "import findspark\n",
    "import zipfile\n",
    "import os\n",
    "from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "import psycopg2\n",
    "# from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/spark\n",
      "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "26/01/11 15:41:51 WARN Utils: Your hostname, r resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/11 15:41:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 15:41:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import findspark\n",
    "# from pyspark.sql import SparkSession\n",
    "print(\"SPARK_HOME:\", os.environ.get('SPARK_HOME'))\n",
    "print(\"JAVA_HOME:\", os.environ.get('JAVA_HOME'))\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"sparksql\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext sparksql_magic\n",
    "# %config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r/git_projekty/sparkTraining/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3493: UserWarning: Config option `max_num_rows` not recognized by `SparkSql`.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparksql_magic\n",
    "%config SparkSql.max_num_rows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general paths\n",
    "DATA = '/home/r/git_projekty/sparkTraining/data'\n",
    "PACKAGED_DATA = '/home/r/git_projekty/sparkTraining/data/packaged'\n",
    "UNPACKAGED_DATA = '/home/r/git_projekty/sparkTraining/data/unpackaged'\n",
    "UNPACKAGED_DATA_FOLDERS = os.listdir(UNPACKAGED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transactions-fraud-datasets', 'csecicids2018', 'us-wildfire-records-6th-edition', 'game-recommendations-on-steam']\n"
     ]
    }
   ],
   "source": [
    "print(UNPACKAGED_DATA_FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_extension_file(filepath):\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    return file_extension.lower()[1:]\n",
    "\n",
    "def check_file_name(filepath):\n",
    "    file_name, _ = os.path.splitext(filepath)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions-fraud-datasets\n",
      "csv cards_data\n",
      "json train_fraud_labels\n",
      "json mcc_codes\n",
      "csv users_data\n",
      "csv transactions_data\n",
      "csecicids2018\n",
      "parquet Botnet-Friday-02-03-2018_TrafficForML_CICFlowMeter\n",
      "parquet DDoS1-Tuesday-20-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet Infil2-Thursday-01-03-2018_TrafficForML_CICFlowMeter\n",
      "parquet DoS2-Friday-16-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet DoS1-Thursday-15-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet Infil1-Wednesday-28-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet Bruteforce-Wednesday-14-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet DDoS2-Wednesday-21-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet Web2-Friday-23-02-2018_TrafficForML_CICFlowMeter\n",
      "parquet Web1-Thursday-22-02-2018_TrafficForML_CICFlowMeter\n",
      "us-wildfire-records-6th-edition\n",
      "csv data\n",
      "csv _variable_descriptions\n",
      "sqlite data\n",
      "game-recommendations-on-steam\n",
      "json games_metadata\n",
      "csv games\n",
      "csv recommendations\n",
      "csv users\n"
     ]
    }
   ],
   "source": [
    "for folder in UNPACKAGED_DATA_FOLDERS:\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(UNPACKAGED_DATA, folder)\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        # print(f'file: {file} file_path {file_path}' )\n",
    "        print(check_extension_file(file), check_file_name(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "df_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data.csv\")\n",
    "\n",
    "# WRITE\n",
    "df_csv.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "df_json = spark.read.json(\"data.json\")\n",
    "\n",
    "# WRITE\n",
    "df_json.write.mode(\"overwrite\").json(\"output_json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "df_parquet = spark.read.parquet(\"data.parquet\")\n",
    "\n",
    "# WRITE (z kompresją snappy - domyślna)\n",
    "df_parquet.write.mode(\"overwrite\").parquet(\"output_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK NEEDS LIBRARY (np. com.crealytics:spark-excel).\n",
    "\n",
    "# READ\n",
    "df_excel = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis jako Delta Table\n",
    "df_parquet.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/table\")\n",
    "\n",
    "# Odczyt konkretnej wersji danych (Time Travel)\n",
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/mnt/delta/table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczyt\n",
    "df_avro = spark.read.format(\"avro\").load(\"data.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Połączono z: ('PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit',)\n",
      "Połączenie zamknięte.\n"
     ]
    }
   ],
   "source": [
    "# import psycopg2\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=\"postgres\",\n",
    "        password=\"password\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=\"5432\",\n",
    "        database=\"postgrestest\"\n",
    "    )\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(f\"Połączono z: {record}\")\n",
    "\n",
    "    # 2. Listowanie baz danych\n",
    "    print(\"Dostępne bazy danych:\")\n",
    "    cursor.execute(\"SELECT datname FROM pg_database WHERE datistemplate = false;\")\n",
    "    databases = cursor.fetchall()\n",
    "\n",
    "    for db in databases:\n",
    "        print(f\" - {db[0]}\")\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"Błąd połączenia: {error}\")\n",
    "\n",
    "finally:\n",
    "    if 'connection' in locals() and connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Połączenie zamknięte.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# from psycopg2 import sql\n",
    "\n",
    "class PostgresManager:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Config powinien być słownikiem z kluczami: user, password, host, port, database\"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def execute_query(self, query, params=None, fetch=False):\n",
    "        \"\"\"Uniwersalna metoda do SELECT, INSERT, UPDATE, DELETE\"\"\"\n",
    "        result = None\n",
    "        try:\n",
    "            with psycopg2.connect(**self.config) as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(query, params)\n",
    "                    if fetch:\n",
    "                        result = cur.fetchall()\n",
    "                    conn.commit()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd bazy danych: {e}\")\n",
    "            return None\n",
    "\n",
    "# --- UŻYCIE W JUPYTERZE ---\n",
    "\n",
    "config = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"postgrestest\"\n",
    "}\n",
    "\n",
    "db = PostgresManager(config)\n",
    "\n",
    "# # Przykład 1: Pobieranie danych (DQL)\n",
    "# dbs = db.execute_query(\"SELECT datname FROM pg_database\", fetch=True)\n",
    "# print(dbs)\n",
    "\n",
    "# # Przykład 2: Tworzenie tabeli (DDL)\n",
    "# db.execute_query(\"CREATE TABLE IF NOT EXISTS users (id serial PRIMARY KEY, name varchar(100));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1: Pobieranie danych (DQL)\n",
    "# dbs = db.execute_query(\"SELECT datname FROM pg_database\", fetch=True)\n",
    "# print(dbs)\n",
    "\n",
    "# # Przykład 2: Tworzenie tabeli (DDL)\n",
    "# db.execute_query(\"CREATE TABLE IF NOT EXISTS users (id serial PRIMARY KEY, name varchar(100));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
